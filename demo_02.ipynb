{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install plotly\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# If Using Google Colab, enable these extensions\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE Loss: A Deep Dive into Gradient Descent\n",
    "\n",
    "Ever wondered how machines learn to predict things? Let's explore one of the coolest techniques: gradient descent with Mean Squared Error (MSE) loss. It's like teaching a computer to play \"Hot and Cold\" with data!\n",
    "\n",
    "### What's the Big Idea?\n",
    "\n",
    "Imagine you're trying to draw the best straight line through a bunch of scattered points. That's linear regression in a nutshell. We use two magic numbers: 'm' (the slope) and 'c' (where the line crosses the y-axis). Our goal? Find the perfect 'm' and 'c' that make our line fit the points best.\n",
    "\n",
    "### How Does It Work?\n",
    "\n",
    "1. **Start with a Guess**: We pick random values for 'm' and 'c'.\n",
    "2. **Check How Bad We Are**: We use MSE to measure how far off our line is from the real points.\n",
    "3. **Take a Step in the Right Direction**: We adjust 'm' and 'c' a tiny bit to make our line fit better.\n",
    "4. **Repeat Until We're Happy**: Keep tweaking until the line barely improves anymore.\n",
    "\n",
    "### Cool Features in Our Simulation\n",
    "\n",
    "- **Tolerance**: It's like saying, \"If we're not getting much better, let's call it a day.\"\n",
    "- **Dynamic Learning**: If we're suddenly doing worse, we slow down our adjustments.\n",
    "- **Gradient Clipping**: Imagine putting guardrails on a mountain road – it keeps us from making crazy changes.\n",
    "\n",
    "### The Secret Sauce: The Loss Surface\n",
    "\n",
    "Picture a 3D landscape where height represents how bad our guess is. Our goal is to find the lowest point in this landscape. For MSE, it looks like a smooth bowl – there's only one bottom to find!\n",
    "\n",
    "### Why Not Just Solve It Directly?\n",
    "\n",
    "Sometimes we can! It's called using \"normal equations.\" But imagine trying to solve a million-piece puzzle all at once – sometimes it's easier to tackle it bit by bit, like gradient descent does.\n",
    "\n",
    "### When Does This Really Shine?\n",
    "\n",
    "- **Big Data**: When you've got tons of points, gradient descent is often faster.\n",
    "- **Tricky Data**: Sometimes the math for solving it directly just doesn't work out. Gradient descent doesn't care – it'll keep chugging along.\n",
    "\n",
    "So there you have it! We're teaching computers to play a super-advanced version of \"Hot and Cold\" to find the best line through our data. Pretty cool, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "def mse(y_true, y_pred):\n",
    "    return sum((t - p)**2 for t, p in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "## Gradient function\n",
    "def grad_mse(t, p):\n",
    "    return -2 * (t - p)\n",
    "\n",
    "def get_y_pred(model, x_data):\n",
    "    return [model.forward(x) for x in x_data]\n",
    "\n",
    "def gradient_descent_step(model, x, t, p, grad_loss, lr, clip_value):\n",
    "    params = vars(model).items()\n",
    "    model_grad = model.grad(x)\n",
    "    for pk, pv in params:\n",
    "        grad = grad_loss(t, p) * model_grad[pk]\n",
    "        # Apply gradient clipping\n",
    "        grad = np.clip(grad, -clip_value, clip_value)\n",
    "        new_pv = pv - lr * grad\n",
    "        setattr(model, pk, new_pv)\n",
    "\n",
    "def create_figure(model, x_data, y_data):\n",
    "    x_min, x_max = min(x_data), max(x_data)\n",
    "    y_min, y_max = min(y_data), max(y_data)\n",
    "    y_pred = get_y_pred(model, x_data)\n",
    "\n",
    "    fig = go.FigureWidget(\n",
    "        data=[\n",
    "            go.Scatter(x=x_data, y=y_data, mode=\"markers\", name=\"Data\", marker=dict(opacity=0.5)),\n",
    "            go.Scatter(x=x_data, y=y_pred, mode=\"lines\", name=\"Regression Line\"),\n",
    "            go.Scatter(\n",
    "                x=[None], y=[None], mode=\"lines\", line=dict(color=\"gray\"), name=\"Error Lines\"\n",
    "            ),\n",
    "        ],\n",
    "        layout=go.Layout(\n",
    "            title=\"Interactive Linear Regression\",\n",
    "            xaxis_title=\"Input\",\n",
    "            yaxis_title=\"Output\",\n",
    "            xaxis_range=[x_min - 2, x_max + 2],\n",
    "            yaxis_range=[y_min - 2, y_max + 2],\n",
    "            xaxis=dict(dtick=5),\n",
    "            yaxis=dict(dtick=5),\n",
    "            autosize=False,\n",
    "            width=16 * 50,\n",
    "            height=9 * 50,\n",
    "            shapes=[\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    xref=\"x\",\n",
    "                    yref=\"y\",\n",
    "                    x0=x,\n",
    "                    y0=y_data[i],\n",
    "                    x1=x,\n",
    "                    y1=y_pred[i],\n",
    "                    line=dict(\n",
    "                        color=\"gray\",\n",
    "                        width=1,\n",
    "                    ),\n",
    "                )\n",
    "                for i, x in enumerate(x_data)\n",
    "            ],\n",
    "            annotations=[\n",
    "                dict(\n",
    "                    text=f\"Mean Squared Error: {mse(y_data, y_pred):.2f}\",\n",
    "                    x=0.5,\n",
    "                    y=0.98,\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"paper\",\n",
    "                    showarrow=False,\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def create_loss_vs_param_figure(model, x_data, y_data):\n",
    "    param_range = np.linspace(-25, 25, 50)\n",
    "    m_values, c_values = np.meshgrid(param_range, param_range)\n",
    "    losses = np.zeros_like(m_values)\n",
    "\n",
    "    for i in range(m_values.shape[0]):\n",
    "        for j in range(m_values.shape[1]):\n",
    "            model.m = m_values[i, j]\n",
    "            model.c = c_values[i, j]\n",
    "            y_pred = get_y_pred(model, x_data)\n",
    "            losses[i, j] = mse(y_data, y_pred)\n",
    "\n",
    "    surface = go.Surface(x=m_values, y=c_values, z=losses, colorscale=\"Viridis\")\n",
    "    scatter = go.Scatter3d(\n",
    "        x=[model.m],\n",
    "        y=[model.c],\n",
    "        z=[mse(y_data, get_y_pred(model, x_data))],\n",
    "        mode=\"markers\",\n",
    "        name=\"Current Parameters\",\n",
    "        marker=dict(color=\"rgba(255, 0, 0, 1)\", size=5),\n",
    "    )\n",
    "\n",
    "    fig = go.FigureWidget(\n",
    "        data=[surface, scatter],\n",
    "        layout=go.Layout(\n",
    "            title=\"Loss vs Model Parameters\",\n",
    "            scene=dict(\n",
    "                xaxis=dict(title=\"m\", showticklabels=False),\n",
    "                yaxis=dict(title=\"c\", showticklabels=False),\n",
    "                zaxis=dict(title=\"Loss\", showticklabels=False),\n",
    "            ),\n",
    "            autosize=False,\n",
    "            width=16 * 75,\n",
    "            height=9 * 75,\n",
    "            margin=dict(l=0, r=0, b=0, t=0),\n",
    "        ),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def plot(x_data, y_data, model):\n",
    "    fig = create_figure(model, x_data, y_data)\n",
    "    loss_vs_param_fig = create_loss_vs_param_figure(model, x_data, y_data)\n",
    "    learning_rate_slider = widgets.FloatLogSlider(value=0.001, base=10, min=-4, max=1, step=0.1, description=\"Learning Rate:\")\n",
    "    start_button = widgets.Button(description=\"Start Simulation\")\n",
    "    max_iterations = widgets.IntText(value=5, description=\"Max Iterations:\")\n",
    "    tol = widgets.FloatText(value=1e-6, description=\"Tolerance:\")\n",
    "    clip_value = widgets.FloatText(value=1.0, description=\"Clip Value:\")\n",
    "\n",
    "    def update_plot(change):\n",
    "        y_pred = get_y_pred(model, x_data)\n",
    "        loss = mse(y_data, y_pred)\n",
    "        with fig.batch_update():\n",
    "            fig.data[1].y = y_pred\n",
    "            fig.layout.shapes = [\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    xref=\"x\",\n",
    "                    yref=\"y\",\n",
    "                    x0=x,\n",
    "                    y0=y_data[i],\n",
    "                    x1=x,\n",
    "                    y1=y_pred[i],\n",
    "                    line=dict(\n",
    "                        color=\"gray\",\n",
    "                        width=2,\n",
    "                    ),\n",
    "                )\n",
    "                for i, x in enumerate(x_data)\n",
    "            ]\n",
    "            fig.layout.annotations[0].text = f\"Mean Squared Error: {loss:.2f}\"\n",
    "\n",
    "        current_loss = mse(y_data, get_y_pred(model, x_data))\n",
    "        scatter = loss_vs_param_fig.data[1]\n",
    "        scatter.x = [model.m]\n",
    "        scatter.y = [model.c]\n",
    "        scatter.z = [current_loss]\n",
    "\n",
    "    def start_simulation(change):\n",
    "        lr = learning_rate_slider.value\n",
    "        max_iter = max_iterations.value\n",
    "        tolerance = tol.value\n",
    "        clip_val = clip_value.value\n",
    "        previous_loss = float('inf')\n",
    "        for iteration in range(max_iter):\n",
    "            y_pred = get_y_pred(model, x_data)\n",
    "            grad_loss = grad_mse\n",
    "            for x, t, p in zip(x_data, y_data, y_pred):\n",
    "                gradient_descent_step(model, x, t, p, grad_loss, lr, clip_val)\n",
    "            update_plot(change)\n",
    "            current_loss = mse(y_data, get_y_pred(model, x_data))\n",
    "            print(f\"Iteration {iteration}: m={model.m}, c={model.c}, loss={current_loss}\")\n",
    "            if abs(previous_loss - current_loss) < tolerance:\n",
    "                break\n",
    "            previous_loss = current_loss\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    start_button.on_click(start_simulation)\n",
    "\n",
    "    return widgets.HBox([widgets.VBox([fig, learning_rate_slider, start_button, max_iterations, tol, clip_value], layout=widgets.Layout(border='solid 2px gray', padding='10px')), widgets.VBox([loss_vs_param_fig], layout=widgets.Layout(margin='0 0 0 -50px'))])\n",
    "\n",
    "class SimpleLinearRegression:\n",
    "    def __init__(self, m=0.7, c=2.2):\n",
    "        self.m = m\n",
    "        self.c = c\n",
    "    \n",
    "    def grad(self, x):\n",
    "        return {\"m\": x, \"c\": 1}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.m * x + self.c\n",
    "\n",
    "# Generate a more interesting dataset with a slight quadratic trend and noise\n",
    "np.random.seed(42)\n",
    "x_data = np.linspace(0, 10, 20)\n",
    "y_data = 2 * x_data + 1 + np.random.normal(0, 1, x_data.shape)\n",
    "\n",
    "model = SimpleLinearRegression()\n",
    "plot_widget = plot(x_data, y_data, model)\n",
    "plot_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE Loss Function in Linear Regression\n",
    "\n",
    "This simulation explores gradient descent for optimizing linear regression parameters by minimizing the Mean Absolute Error (MAE) loss. Unlike Mean Squared Error (MSE), MAE uses absolute error values, reducing sensitivity to outliers.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **MAE Loss Function**: Average of absolute differences between predicted and true values.\n",
    "2. **Gradient Calculation**: For MAE, the gradient is -1 if the true value exceeds the predicted value, and 1 otherwise.\n",
    "3. **Linear Regression Parameters**:\n",
    "   - m: Slope\n",
    "   - c: Intercept\n",
    "   - Model: y = m * x + c\n",
    "\n",
    "4. **Function Surface**:\n",
    "   - Represents loss as a function of m and c\n",
    "   - MAE surface has a characteristic \"V\" shape\n",
    "\n",
    "### Gradient Descent for MAE\n",
    "\n",
    "The algorithm updates parameters by moving opposite to the loss function's gradient. Gradient clipping prevents excessive updates.\n",
    "\n",
    "### Visualization Features\n",
    "\n",
    "1. **Loss Contours**:\n",
    "   - Horizontal lines on the m axis show regions of constant loss\n",
    "   - Reflect the \"V\" shape of the MAE surface\n",
    "   - Aid in visualizing the gradient descent path\n",
    "\n",
    "2. **Gradient Descent Path**:\n",
    "   - Red dot shows optimization progress\n",
    "   - Moves along contours towards minimum loss\n",
    "\n",
    "### MAE vs. MSE\n",
    "\n",
    "- MAE surface has linear sections forming a \"V\" shape\n",
    "- MSE surface is smooth and convex\n",
    "\n",
    "This simulation demonstrates gradient descent with MAE, showcasing convergence to optimal parameters and comparing it with normal equations. The visualization of the loss surface and parameter updates provides insights into gradient descent's behavior and advantages in various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "def mae(y_true, y_pred):\n",
    "    return sum(abs(t - p) for t, p in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "## Gradient function\n",
    "def grad_mae(t, p):\n",
    "    return -1 if t > p else 1\n",
    "\n",
    "def get_y_pred(model, x_data):\n",
    "    return [model.forward(x) for x in x_data]\n",
    "\n",
    "def gradient_descent_step(model, x, t, p, grad_loss, lr, clip_value):\n",
    "    params = vars(model).items()\n",
    "    model_grad = model.grad(x)\n",
    "    for pk, pv in params:\n",
    "        grad = grad_loss(t, p) * model_grad[pk]\n",
    "        # Apply gradient clipping\n",
    "        grad = np.clip(grad, -clip_value, clip_value)\n",
    "        new_pv = pv - lr * grad\n",
    "        setattr(model, pk, new_pv)\n",
    "\n",
    "def create_figure(model, x_data, y_data):\n",
    "    x_min, x_max = min(x_data), max(x_data)\n",
    "    y_min, y_max = min(y_data), max(y_data)\n",
    "    y_pred = get_y_pred(model, x_data)\n",
    "\n",
    "    fig = go.FigureWidget(\n",
    "        data=[\n",
    "            go.Scatter(x=x_data, y=y_data, mode=\"markers\", name=\"Data\", marker=dict(opacity=0.5)),\n",
    "            go.Scatter(x=x_data, y=y_pred, mode=\"lines\", name=\"Regression Line\"),\n",
    "            go.Scatter(\n",
    "                x=[None], y=[None], mode=\"lines\", line=dict(color=\"gray\"), name=\"Error Lines\"\n",
    "            ),\n",
    "        ],\n",
    "        layout=go.Layout(\n",
    "            title=\"Interactive Linear Regression\",\n",
    "            xaxis_title=\"Input\",\n",
    "            yaxis_title=\"Output\",\n",
    "            xaxis_range=[x_min - 2, x_max + 2],\n",
    "            yaxis_range=[y_min - 2, y_max + 2],\n",
    "            xaxis=dict(dtick=5),\n",
    "            yaxis=dict(dtick=5),\n",
    "            autosize=False,\n",
    "            width=16 * 50,\n",
    "            height=9 * 50,\n",
    "            shapes=[\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    xref=\"x\",\n",
    "                    yref=\"y\",\n",
    "                    x0=x,\n",
    "                    y0=y_data[i],\n",
    "                    x1=x,\n",
    "                    y1=y_pred[i],\n",
    "                    line=dict(\n",
    "                        color=\"gray\",\n",
    "                        width=1,\n",
    "                    ),\n",
    "                )\n",
    "                for i, x in enumerate(x_data)\n",
    "            ],\n",
    "            annotations=[\n",
    "                dict(\n",
    "                    text=f\"Mean Absolute Error: {mae(y_data, y_pred):.2f}\",\n",
    "                    x=0.5,\n",
    "                    y=0.98,\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"paper\",\n",
    "                    showarrow=False,\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def create_loss_vs_param_figure(model, x_data, y_data):\n",
    "    param_range = np.linspace(-25, 25, 50)\n",
    "    m_values, c_values = np.meshgrid(param_range, param_range)\n",
    "    losses = np.zeros_like(m_values)\n",
    "\n",
    "    for i in range(m_values.shape[0]):\n",
    "        for j in range(m_values.shape[1]):\n",
    "            model.m = m_values[i, j]\n",
    "            model.c = c_values[i, j]\n",
    "            y_pred = get_y_pred(model, x_data)\n",
    "            losses[i, j] = mae(y_data, y_pred)\n",
    "\n",
    "    surface = go.Surface(x=m_values, y=c_values, z=losses, colorscale=\"Viridis\", contours={\"z\": {\"show\": True, \"usecolormap\": True, \"highlightcolor\": \"limegreen\", \"project\": {\"z\": True}}})\n",
    "    scatter = go.Scatter3d(\n",
    "        x=[model.m],\n",
    "        y=[model.c],\n",
    "        z=[mae(y_data, get_y_pred(model, x_data))],\n",
    "        mode=\"markers\",\n",
    "        name=\"Current Parameters\",\n",
    "        marker=dict(color=\"rgba(255, 0, 0, 1)\", size=5),\n",
    "    )\n",
    "\n",
    "    fig = go.FigureWidget(\n",
    "        data=[surface, scatter],\n",
    "        layout=go.Layout(\n",
    "            title=\"Loss vs Model Parameters\",\n",
    "            scene=dict(\n",
    "                xaxis=dict(title=\"m\", showticklabels=False),\n",
    "                yaxis=dict(title=\"c\", showticklabels=False),\n",
    "                zaxis=dict(title=\"Loss\", showticklabels=False),\n",
    "            ),\n",
    "            autosize=False,\n",
    "            width=16 * 75,\n",
    "            height=9 * 75,\n",
    "            margin=dict(l=0, r=0, b=0, t=0),\n",
    "        ),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def plot(x_data, y_data, model):\n",
    "    fig = create_figure(model, x_data, y_data)\n",
    "    loss_vs_param_fig = create_loss_vs_param_figure(model, x_data, y_data)\n",
    "    learning_rate_slider = widgets.FloatLogSlider(value=0.001, base=10, min=-4, max=1, step=0.1, description=\"Learning Rate:\")\n",
    "    start_button = widgets.Button(description=\"Start Simulation\")\n",
    "    max_iterations = widgets.IntText(value=5, description=\"Max Iterations:\")\n",
    "    tol = widgets.FloatText(value=1e-6, description=\"Tolerance:\")\n",
    "    clip_value = widgets.FloatText(value=1.0, description=\"Clip Value:\")\n",
    "\n",
    "    def update_plot(change):\n",
    "        y_pred = get_y_pred(model, x_data)\n",
    "        loss = mae(y_data, y_pred)\n",
    "        with fig.batch_update():\n",
    "            fig.data[1].y = y_pred\n",
    "            fig.layout.shapes = [\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    xref=\"x\",\n",
    "                    yref=\"y\",\n",
    "                    x0=x,\n",
    "                    y0=y_data[i],\n",
    "                    x1=x,\n",
    "                    y1=y_pred[i],\n",
    "                    line=dict(\n",
    "                        color=\"gray\",\n",
    "                        width=2,\n",
    "                    ),\n",
    "                )\n",
    "                for i, x in enumerate(x_data)\n",
    "            ]\n",
    "            fig.layout.annotations[0].text = f\"Mean Absolute Error: {loss:.2f}\"\n",
    "\n",
    "        current_loss = mae(y_data, get_y_pred(model, x_data))\n",
    "        scatter = loss_vs_param_fig.data[1]\n",
    "        scatter.x = [model.m]\n",
    "        scatter.y = [model.c]\n",
    "        scatter.z = [current_loss]\n",
    "\n",
    "    def start_simulation(change):\n",
    "        lr = learning_rate_slider.value\n",
    "        max_iter = max_iterations.value\n",
    "        tolerance = tol.value\n",
    "        clip_val = clip_value.value\n",
    "        previous_loss = float('inf')\n",
    "        for iteration in range(max_iter):\n",
    "            y_pred = get_y_pred(model, x_data)\n",
    "            grad_loss = grad_mae\n",
    "            for x, t, p in zip(x_data, y_data, y_pred):\n",
    "                gradient_descent_step(model, x, t, p, grad_loss, lr, clip_val)\n",
    "            update_plot(change)\n",
    "            current_loss = mae(y_data, get_y_pred(model, x_data))\n",
    "            print(f\"Iteration {iteration}: m={model.m}, c={model.c}, loss={current_loss}\")\n",
    "            if abs(previous_loss - current_loss) < tolerance:\n",
    "                break\n",
    "            previous_loss = current_loss\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    start_button.on_click(start_simulation)\n",
    "\n",
    "    return widgets.HBox([widgets.VBox([fig, learning_rate_slider, start_button, max_iterations, tol, clip_value], layout=widgets.Layout(border='solid 2px gray', padding='10px')), widgets.VBox([loss_vs_param_fig], layout=widgets.Layout(margin='0 0 0 -50px'))])\n",
    "\n",
    "class SimpleLinearRegression:\n",
    "    def __init__(self, m=0.7, c=2.2):\n",
    "        self.m = m\n",
    "        self.c = c\n",
    "    \n",
    "    def grad(self, x):\n",
    "        return {\"m\": x, \"c\": 1}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.m * x + self.c\n",
    "\n",
    "# Generate a more interesting dataset with a slight quadratic trend and noise\n",
    "np.random.seed(42)\n",
    "x_data = np.linspace(0, 10, 20)\n",
    "y_data = 2 * x_data + 1 + np.random.normal(0, 1, x_data.shape)\n",
    "\n",
    "model = SimpleLinearRegression()\n",
    "plot_widget = plot(x_data, y_data, model)\n",
    "plot_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
